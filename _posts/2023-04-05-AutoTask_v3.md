---
title: AutoTask_v1
layout: post
---

Com que a la universitat cada vegada tinc més entregues a realitzar, he decidit començar a organitzar-me, i a arrel d'això apareix aquest projecte. Bàsicament la idea és crear un programa que llegeixi totes les tasques i entregues que s'han de fer durant la setmana i me les guardi, ja sigui en un calendari o en un To-Do.


## API's i Llibreries

Després d'haver fet una cerca inicial, i degut a que ja coneixia la llibreria, he decidit utilitzar la llibreria de [Selenium] de python per a fer el web scrapping, i ja posteriorment, quan el programa sigui funcional, tinc pensat en utilitzar altres opcions com [requests] per a fer-lo més òptim, però per a soritr del pas utilitzaré selenium.

Per a guardar les entregeus en un principi vaig decidir utilitzar [Google Calendar], però com que la seva API només permetia crear esdeveniments i no tasques, també utilitzare la API de [Google Tasks]

[Selenium]: https://www.selenium.dev/
[requests]:   https://pypi.org/project/requests/
[Google Calendar]: https://developers.google.com/calendar/api/guides/overview
[Google Tasks]: https://developers.google.com/tasks



### Primers Passos

Primer de tot, el que faré es provar a connectar-me al [Campus Virtual] (CV) de la universitat. Per a fer-ho importar-e les llibreries necessàries:

[Campus Virtual]: https://cv.uab.cat/

```python

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

```
Selenium permet utilitzar diferents navegadors, però en el meu cas, com que cada vegada que entro al CV m'he de registrar, el que faré és guardar les dades en una sessió, i així evitar el login cada vegada que vulgui executar el programa. Un navegador que permet fer fàcilemnt això és Google Chrome. 

Per tant, descarregaré el webdriver de chrome i li indicaré el directori del chromedriver al programa.

```python
from webdriver_manager.chrome import ChromeDriverManager

CHROME_PATH = "/home/guerau/Documents/UAB/Tercer/horari/chromedriver/chromedriver"

```
Abans de fer la crida del nostre `driver` hem de establir-ne les opcions. En aquest cas, les opcions que he ficat són per a que no es mostrin gaires logs, i per a especificar-ne el usuari.

```python
options = webdriver.ChromeOptions()
options.add_argument('--headless=new')

options.add_argument('--disable-gpu')
options.add_argument('--no-sandbox')

options.add_argument('--log-level=3')
options.add_argument('--user-data-dir=/home/guerau/.config/chromium/')
options.add_argument('--profile-directory=Profile 1')

```

> Per saber el directori del usuari, a la barra de navegació de Google Chrome sha de posar `chrome://options`


### Primera Crida del Navegador

Ara que ja ho tinc tot preparat, ja puc fer la crida al horari del CV de la universitat:


```python
driver = webdriver.Chrome(executable_path=CHROME_PATH, options=options)
wait = WebDriverWait(driver, 20)


url = "https://e-aules.uab.cat/2022-23/calendar/view.php?view=upcoming"
driver.get(url)

```

En aquest cas, el `driver` serà amb el que treballarem tota l'estona, serà l'objecte que ens mostri el contingut html de la pàgina, i podrem cridar a funcions per a buscar elements tant pel seu identificador, la classe, el nom, o en el meu cas, el XPATH. El que faré és buscar el XPATH del botó de registrar-se inspeccionant la pàgina html, i amb els mètodes del `driver` hi faré click. 

> XPath és un llenguatge que permet construir expressions que recuperen i processen un document XML. Ho utilitzarem per a identificar els objectes, perque tot i que canviin de identificador, el XPATH serà el mateix.


```python
wait.until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[2]/main/div/div/div[2]/div/a'))).click()
```


### Cookies i Requests 

Una vegada ja entri a la pàgina del CV automàticament, i m'hi registri, el que hauré de fer és llegir el contingut de la pàgina. El contingut es pot llegir des del mateix `driver`amb el atribut `.text` dels objectes. No obstant, com que no està organitzat d'una manera ordenada, he decidit llegir les dades amb requests, les quals em retornen directament el còdi html, i més més senzill extreure'n els valors.

El més lògic és pensar que el més efectiu hauria sigut començar des del principi fent `requests` enlloc d'utilitzar selènium, però per a que els requests funcionin necessiten tenir unes Cookies vàlides. El problema és que el CV assigna unes Cookies amb una data de caducitat d'entre dos a tres hores, i que per tant, no és sempre la mateixa, i no em serviria de res guardar-les en un fitxer. Per tant, la solució que he trobat ha sigut utilitzar el `selenium` per registrar-me, copiar les Cookies del selenium, i afegir-les al `request`, així cada vegada que executi el còdi se'm generaran unes cookies valides.


```python

ck = driver.get_cookies()
#print('COOKES: ', ck)
s = requests.Session()
c = [s.cookies.set(c['name'], c['value']) for c in ck]
response = s.get(url)
```
> Per a fer els `requests` és necessaria la llibreria de requests, que s'importa de la següent manera: `import requests` 

### Obtenció de Dades

Finalment, el "scrapping" de la web el faré amb `Beautiful Soap` una altra llibreria de python, dedicada al scrapping web.


```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(response.text, 'html.parser')

# Filtrem per la classe "event"
events = soup.select('.event')

# Per cada event, organitzem les dades en un diccinari {'titol', 'data', 'assignatura', 'descripcció'}
data = []
for event in events:
    event_data = {}
    event_data['title'] = event.select_one('.name').text

    rows = event.select('.row')
    for row in rows:
        if(row.select_one('.icon').attrs['title'] == 'Quan'):
            event_data['date'] = row.select_one('.col-11').text
        elif(row.select_one('.icon').attrs['title'] == 'Curs'):
            event_data['subject'] = row.select_one('.col-11').text
        elif(row.select_one('.icon').attrs['title'] == 'Descripció'):
            event_data['description'] = row.select_one('.col-11').text

    #data.append(event_data)
    if 'description' not in event_data:
        event_data['description'] = ''
    if 'subject' not in event_data:
        event_data['subject'] = ''


    data.append(event_data)
```

Una vegada ja tinc les dades llegides i organitzades de la Web, les hauré de posar en les API's de Google, però això ho deixo pel següent post.
